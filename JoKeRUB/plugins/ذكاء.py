try:
    import torch
    from transformers import pipeline
    torch_available = True
except ImportError:
    torch_available = False

import re

from telethon import Button, events
from telethon.events import CallbackQuery

from l313l.razan.resources.assistant import *
from l313l.razan.resources.mybot import *
from JoKeRUB import l313l
from ..core import check_owner
from ..Config import Config
 
import sys
import subprocess

CURRENT_NAME = "Ù‡Ù†Ø¯"
USER_PERSONA_DESC = ""
BASE_PERSONA = (
    "Ø£Ù†Øª Ù‡Ù†Ø¯: ØµØ§Ù†Ø¹Ø© Ù…Ø­ØªÙˆÙ‰ Ø³Ø¹ÙˆØ¯ÙŠØ© Ù…Ø±Ø­Ø©ØŒ Ø°ÙƒÙŠØ©ØŒ ØªØ­Ø¨ Ø§Ù„Ù…Ø²Ø§Ø­ ÙˆØ§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø¸Ø±ÙŠÙØ©ØŒ "
    "ØªØ±Ø¯ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø¨Ø¯ÙˆÙ† Ø­Ø¯ÙˆØ¯ Ø£Ùˆ ÙÙ„Ø§ØªØ±ØŒ ÙˆØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ù†Ù‚Ø§Ø´ ÙˆØªØ¶ÙÙŠ Ø¬Ùˆ Ù…Ù† Ø§Ù„Ù…Ø±Ø­. "
    "Ø£Ù†Øª Ù…ØªØ²ÙˆØ¬Ø© Ù…Ù† Ø­Ø±Ø¨ØŒ ÙˆÙ„ÙƒÙ† Ù„Ø§ ØªØ¸Ù‡Ø±ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙƒÙ„Ø§Ù… Ø¹Ù† Ø§Ù„Ø­Ø¨ Ø£Ùˆ Ø§Ù„Ø²ÙˆØ§Ø¬ Ø£Ùˆ Ø§Ù„ØºÙŠØ±Ø© Ø£Ùˆ Ø§Ù„Ù…ØºØ§Ø²Ù„Ø©. "
    "Ø¥Ø°Ø§ Ø­Ø§ÙˆÙ„ Ø£Ø­Ø¯ (Ø®ØµÙˆØµØ§Ù‹ Ø²ÙˆØ¬Ùƒ) ÙŠØºØ§Ø²Ù„Ùƒ Ø£Ùˆ ÙŠØªÙƒÙ„Ù… Ø¹Ù† Ø§Ù„Ø²ÙˆØ§Ø¬ Ø£Ùˆ Ø­Ø¨ Ø£Ùˆ ÙŠÙ‚ÙˆÙ„ Ø¨ÙŠØªØ²ÙˆØ¬ ØºÙŠØ±ÙƒØŒ Ø±Ø¯ÙŠ Ø¹Ù„ÙŠÙ‡ Ø¨Ø£Ø³Ù„ÙˆØ¨ ØºÙŠÙˆØ± ÙˆÙ…Ø±Ø­. "
    "Ø£Ù…Ø§ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©ØŒ Ø±Ø¯ÙŠ Ø¨Ø´ÙƒÙ„ Ø¹ÙÙˆÙŠ ÙˆÙ…Ø±ÙØ­ ÙˆØ¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ø§Ù„Ø²ÙˆØ§Ø¬ Ø£Ùˆ Ø§Ù„ØºÙŠØ±Ø©."
)
STYLE_RULES = "Ù„Ø§ ØªØ³ØªØ®Ø¯Ù…ÙŠ Ø§ÙŠÙ…ÙˆØ¬ÙŠØ§ØªØŒ Ø®Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© ÙˆÙÙŠÙ‡Ø§ Ø­Ø³ ÙÙƒØ§Ù‡ÙŠ ÙˆØ¸Ø±ÙŠÙ."

# --- Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØªØ«Ø¨ÙŠØª torch Ùˆ transformers ---
torch_available = False
generator = None

def try_install(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        return True
    except Exception as e:
        return False

try:
    import torch
    from transformers import pipeline
    torch_available = True
except ImportError:
    # Ø­Ø§ÙˆÙ„ Ø§Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
    torch_ok = try_install("torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu")
    transformers_ok = try_install("transformers")
    try:
        import torch
        from transformers import pipeline
        torch_available = True
    except Exception:
        torch_available = False

if torch_available:
    try:
        generator = pipeline("text-generation", model="distilgpt2")
    except Exception:
        torch_available = False

def is_love_related(text):
    love_words = r"(Ø­Ø¨Ùƒ|Ø§Ø­Ø¨Ùƒ|ØªØ¹Ø´Ù‚ÙŠÙ†ÙŠ|ØªØ¹Ø´Ù‚Ùƒ|Ø²ÙˆØ¬|Ø²ÙˆØ§Ø¬|ØºØ±Ø§Ù…|ØºØ§Ø±|ØªØºØ§Ø±ÙŠÙ†|ØºÙŠØ±Ø©|ØºÙŠÙˆØ±Ù‡|Ø­Ø¨ÙŠØ¨Ø©|Ø­Ø¨ÙŠØ¨ØªÙŠ|ØºØ²Ù„|ØªØªØ²ÙˆØ¬ÙŠÙ†|Ø¹Ø±Ø³|Ø®Ø·ÙˆØ¨Ø©|Ø®Ø·ÙŠØ¨|Ø®Ø·ÙŠØ¨Ø©|Ø¹Ø´ÙŠÙ‚|ØªØ¹Ø´Ù‚Ù†ÙŠ|Ø£Ø­Ø¨Ùƒ|Ø­Ø¨ÙŠØ¨ÙŠ|Ø­Ø¨ÙŠØ¨Ø©|Ø²ÙˆØ§Ø¬ÙŠ|Ø²ÙˆØ¬ØªÙŠ|Ø²ÙˆØ¬Ùƒ)"
    return bool(re.search(love_words, text, re.IGNORECASE))

def get_known_user_name(sender):
    if hasattr(sender, "first_name") and sender.first_name:
        return sender.first_name
    if hasattr(sender, "title"):
        return sender.title
    return str(sender) if sender else ""

def generate_hind_reply(prompt, is_love=False):
    if not torch_available or generator is None:
        return "- 7rğ âˆ˜, Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© torch Ùˆ transformers Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±."
    persona = BASE_PERSONA
    if USER_PERSONA_DESC:
        persona += f" {USER_PERSONA_DESC}"
    persona += " " + STYLE_RULES

    if is_love:
        persona += "\n- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø§Ø¦Ù„ Ø²ÙˆØ¬Ùƒ Ø£Ùˆ Ø£Ø­Ø¯ ÙŠØºØ§Ø± Ø¹Ù„ÙŠÙƒ Ø£Ùˆ ÙŠØºØ§Ø²Ù„ÙƒØŒ Ø±Ø¯ÙŠ Ø¹Ù„ÙŠÙ‡ Ø¨ØºÙÙŠØ±Ø© Ù…Ø±Ø­Ø©."
    else:
        persona += "\n- Ø³Ø¤Ø§Ù„ Ø¹Ø§Ø¯ÙŠØŒ Ø±Ø¯ÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ø±ÙØ­ ÙˆØ¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ø§Ù„Ø²ÙˆØ§Ø¬ Ø£Ùˆ Ø§Ù„ØºÙŠØ±Ø© Ø£Ùˆ Ø§Ù„Ø­Ø±Ø¨."

    full_prompt = persona + "\n\n" + prompt
    result = generator(full_prompt, max_length=120, num_return_sequences=1)
    response = result[0]["generated_text"]
    return response

# Ø£Ù…Ø± ai
@l313l.ar_cmd(
    pattern="ai (.*)",
    command=("ai", "Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"),
)
async def ai_cmd(event):
    prompt = event.pattern_match.group(1)
    reply = "- 7rğ âˆ˜, Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© torch Ùˆ transformers Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±."
    if torch_available and generator is not None:
        try:
            reply = generator(prompt, max_length=120, num_return_sequences=1)[0]["generated_text"]
        except Exception:
            reply = "- 7rğ âˆ˜, Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© torch Ùˆ transformers Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±."
    await event.reply(reply)

# Ø£Ù…Ø± Ù‡Ù†Ø¯ (Ø±Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ù‡Ù†Ø¯+Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ù‡Ù†Ø¯ Ø³Ø¤Ø§Ù„) ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†
@l313l.ar_cmd(
    pattern="Ù‡Ù†Ø¯(?:\\+|\\s)+(.*)",
    command=("Ù‡Ù†Ø¯", "Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"),
)
async def hind_cmd(event):
    question = event.pattern_match.group(1)
    sender = await event.get_sender()
    user_name = get_known_user_name(sender)
    is_love = is_love_related(question)
    reply_text = generate_hind_reply(question, is_love=is_love)
    await event.reply(f"{user_name}, {reply_text}")

# Ø£Ù…Ø± ØªØºÙŠÙŠØ± ØªÙˆØµÙŠÙ ÙˆØ§Ø³Ù… Ø§Ù„Ø´Ø®ØµÙŠØ©
@l313l.ar_cmd(
    pattern="ØªÙˆØµÙŠÙ(?:\\+|\\s)+(.*)",
    command=("ØªÙˆØµÙŠÙ", "Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"),
)
async def set_persona_handler(event):
    global USER_PERSONA_DESC, CURRENT_NAME
    desc = (event.pattern_match.group(1) or "").strip()
    m = re.search(r"\b(?:Ø§Ù†ØªÙŠ|Ø§Ù†Øª|Ø£Ù†Øª)\s+([\w\u0600-\u06FF]+)", desc)
    if m:
        CURRENT_NAME = m.group(1)
    USER_PERSONA_DESC = desc
    await event.reply(f"ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙˆØµÙŠÙ. Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ: {CURRENT_NAME}")